

1. The algorithm made it seem like the actual image was more accurate to the left image than the right. The output image looked like it was a copy of the left image. It also mistakenly identifies other things as close by (such as buildings on two sides of the sticky note).

2. With the real2 image, the algorithm definitely works worse due to low contrast. Real 2 has lower contrast ratio compared to real 1, which causes the algorithm to falsely identify similar features.The background seems to be as close as the sticky note.  

3. The Cal logo gradually becomes more vague and gets blurrier. This happens because as we increase the feature's width and height, the number of iterations the program passes through lessens. With a greater feature height/width, there is less room for the feature patch to move around because the max displacement remains constant. With more iterations - more movement of the feature patch - we are able to add in more iterative and recursive calls. Though a lower heigh/width may take more time to execute, the repetitive calls pose a more precise representation of the image. Also from the perspective of squared distance, when we have a smaller feature width and height, we actually get larger patch, which means the squared distance is calculated based on the patch. When feature width and height both equals to zero, we actually compare every single pixel, but when they get larger, we calculate the squared distance for a patch and compare with the original image. There are more chances for pixels in patch to be different from the orginal one when we have a bigger patch. That's why it gets blurrier and less acurrate.   